"""
Google Gemini LLM Client
"""
import os
import logging
from typing import List, Dict
import google.generativeai as genai

logger = logging.getLogger(__name__)


class GeminiClient:
    """Client ƒë·ªÉ giao ti·∫øp v·ªõi Google Gemini API"""
    
    def __init__(self, api_key: str | None = None):
        """
        Initialize Gemini client
        
        Args:
            api_key: Gemini API key. N·∫øu None, s·∫Ω l·∫•y t·ª´ env GEMINI_API_KEY
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        
        if not self.api_key:
            raise ValueError(
                "GEMINI_API_KEY not found. "
                "Get your free API key from: https://makersuite.google.com/app/apikey"
            )
        
        # Configure Gemini
        genai.configure(api_key=self.api_key)
        
        # Use Gemini 2.0 Flash model (latest, fast and free)
        self.model = genai.GenerativeModel('gemini-2.0-flash')
        
        # Configure generation settings
        self.generation_config = {
            'temperature': 0.7,  # V·ª´a s√°ng t·∫°o v·ª´a consistent
            'top_p': 0.9,
            'top_k': 40,
            'max_output_tokens': 800,  # ƒê·ªß cho response chi ti·∫øt nh∆∞ng kh√¥ng qu√° d√†i
        }
        
        # Safety settings - cho ph√©p medical content
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_ONLY_HIGH"  # Cho ph√©p medical content
            },
        ]
        
        logger.info("‚úÖ Gemini client initialized successfully")
    
    async def chat(
        self, 
        messages: List[Dict[str, str]], 
        temperature: float | None = None
    ) -> str:
        """
        G·ª≠i messages v√† nh·∫≠n response t·ª´ Gemini
        
        Args:
            messages: List of messages in OpenAI format
                [
                    {"role": "system", "content": "You are..."},
                    {"role": "user", "content": "Hello"},
                    {"role": "assistant", "content": "Hi!"},
                    {"role": "user", "content": "How are you?"}
                ]
            temperature: Override default temperature
        
        Returns:
            str: Response t·ª´ Gemini
        
        Raises:
            Exception: N·∫øu c√≥ l·ªói khi call API
        """
        try:
            # Convert OpenAI-style messages to Gemini format
            gemini_messages = self._convert_messages_to_gemini_format(messages)
            
            # Override temperature if provided
            config = self.generation_config.copy()
            if temperature is not None:
                config['temperature'] = temperature
            
            # Start chat session
            chat = self.model.start_chat(history=gemini_messages['history'])
            
            # Send last user message
            response = chat.send_message(
                gemini_messages['last_message'],
                generation_config=config,
                safety_settings=self.safety_settings
            )
            
            # Check if response was blocked
            if not response.text:
                logger.warning("‚ö†Ô∏è Gemini response was blocked by safety filters")
                return (
                    "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y do ch√≠nh s√°ch an to√†n. "
                    "N·∫øu b·∫°n c√≥ th·∫Øc m·∫Øc v·ªÅ s·ª©c kh·ªèe, vui l√≤ng tham kh·∫£o b√°c sƒ© chuy√™n khoa. üè•"
                )
            
            logger.info(f"‚úÖ Gemini response received ({len(response.text)} chars)")
            return response.text
            
        except Exception as e:
            logger.error(f"‚ùå Gemini API error: {str(e)}")
            
            # User-friendly error message
            if "API_KEY_INVALID" in str(e):
                return (
                    "‚ö†Ô∏è L·ªói c·∫•u h√¨nh API. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n. "
                    "(Gemini API key kh√¥ng h·ª£p l·ªá)"
                )
            elif "RESOURCE_EXHAUSTED" in str(e):
                return (
                    "‚ö†Ô∏è H·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng th·ª≠ l·∫°i sau v√†i ph√∫t. "
                    "(ƒê√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n requests)"
                )
            else:
                return (
                    f"‚ö†Ô∏è Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n. "
                    f"Vui l√≤ng th·ª≠ l·∫°i sau. (L·ªói: {str(e)[:100]})"
                )
    
    def _convert_messages_to_gemini_format(self, messages: List[Dict[str, str]]) -> Dict:
        """
        Convert OpenAI-style messages to Gemini chat format
        
        Gemini format:
        - history: List of {"role": "user", "parts": ["text"]} and {"role": "model", "parts": ["text"]}
        - last_message: String (user's last message)
        
        Args:
            messages: OpenAI-style messages
        
        Returns:
            Dict with 'history' and 'last_message'
        """
        history = []
        system_prompt = ""
        
        for msg in messages:
            role = msg['role']
            content = msg['content']
            
            if role == 'system':
                # Gemini kh√¥ng c√≥ "system" role, n√™n g·ªôp v√†o first user message
                system_prompt += content + "\n\n"
            
            elif role == 'user':
                history.append({
                    "role": "user",
                    "parts": [content]
                })
            
            elif role == 'assistant':
                history.append({
                    "role": "model",  # Gemini g·ªçi l√† "model" thay v√¨ "assistant"
                    "parts": [content]
                })
        
        # L·∫•y last user message
        if history and history[-1]['role'] == 'user':
            last_message = history[-1]['parts'][0]
            history = history[:-1]  # Remove last message from history
        else:
            last_message = "Xin ch√†o!"
        
        # Prepend system prompt to first user message n·∫øu c√≥
        if system_prompt and history and history[0]['role'] == 'user':
            history[0]['parts'][0] = system_prompt + history[0]['parts'][0]
        elif system_prompt:
            # N·∫øu ch∆∞a c√≥ user message, t·∫°o m·ªôt message m·ªõi
            last_message = system_prompt + last_message
        
        return {
            'history': history,
            'last_message': last_message
        }
    
    def test_connection(self) -> bool:
        """
        Test xem Gemini API c√≥ ho·∫°t ƒë·ªông kh√¥ng
        
        Returns:
            bool: True n·∫øu OK, False n·∫øu c√≥ l·ªói
        """
        try:
            response = self.model.generate_content(
                "Say 'OK' if you can read this.",
                generation_config={'max_output_tokens': 10}
            )
            return response.text is not None
        except Exception as e:
            logger.error(f"‚ùå Gemini connection test failed: {e}")
            return False
