# ü§ñ H∆∞·ªõng d·∫´n Tri·ªÉn khai Chatbot AI cho DermaSafe

## üìã M·ª•c l·ª•c
1. [T·ªïng quan ki·∫øn tr√∫c](#t·ªïng-quan-ki·∫øn-tr√∫c)
2. [L·ª±a ch·ªçn c√¥ng ngh·ªá](#l·ª±a-ch·ªçn-c√¥ng-ngh·ªá)
3. [Implementation Steps](#implementation-steps)
4. [Code Examples](#code-examples)
5. [Deployment](#deployment)

---

## üèóÔ∏è T·ªïng quan ki·∫øn tr√∫c

### Lu·ªìng ho·∫°t ƒë·ªông
```
1. User upload ·∫£nh ‚Üí AI ph√¢n t√≠ch ‚Üí Tr·∫£ v·ªÅ k·∫øt qu·∫£
2. User click "Chat v·ªõi AI" ‚Üí M·ªü chatbot
3. Chatbot nh·∫≠n context t·ª´ k·∫øt qu·∫£ ph√¢n t√≠ch
4. User h·ªèi ƒë√°p ‚Üí Chatbot tr·∫£ l·ªùi d·ª±a tr√™n:
   - K·∫øt qu·∫£ AI analysis
   - Medical knowledge base
   - User conversation history
```

### Ki·∫øn tr√∫c ƒë·ªÅ xu·∫•t

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FRONTEND                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ImageUploader‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ  ChatInterface   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ       ‚îÇ  (m·ªõi th√™m)      ‚îÇ    ‚îÇ
‚îÇ                        ‚ñº       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ             ‚îÇ
‚îÇ                  ‚îÇResultCard‚îÇ            ‚îÇ             ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ                    ‚îÇ
                     ‚ñº                    ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Backend API  ‚îÇ    ‚îÇ Chatbot Service  ‚îÇ
              ‚îÇ  Port: 8000  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÇ   Port: 8002     ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ                     ‚îÇ
                     ‚ñº                     ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
              ‚îÇ AI Service   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ  Port: 8001  ‚îÇ  (Get analysis context)
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ LLM API Call
                     ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ OpenAI GPT-4 ‚îÇ
              ‚îÇ ho·∫∑c Gemini  ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è L·ª±a ch·ªçn c√¥ng ngh·ªá

### Option 1: OpenAI GPT-4 (Khuy·∫øn ngh·ªã cho Production)

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Hi·ªÉu ti·∫øng Vi·ªát c·ª±c t·ªët
- ‚úÖ Response ch·∫•t l∆∞·ª£ng cao, empathetic
- ‚úÖ API ·ªïn ƒë·ªãnh, rate limit cao
- ‚úÖ C√≥ function calling (g·ªçi API n·ªôi b·ªô)
- ‚úÖ H·ªó tr·ª£ streaming (real-time response)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Chi ph√≠: ~$0.01/1K tokens (input), $0.03/1K tokens (output)
- ‚ùå C·∫ßn API key (tr·∫£ ph√≠)

**Use case:** S·∫£n ph·∫©m production, c·∫ßn ch·∫•t l∆∞·ª£ng cao

---

### Option 2: Google Gemini Pro (Free tier t·ªët)

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Free tier: 60 requests/minute
- ‚úÖ Hi·ªÉu ti·∫øng Vi·ªát t·ªët
- ‚úÖ Response nhanh
- ‚úÖ H·ªó tr·ª£ multimodal (text + image)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Free tier gi·ªõi h·∫°n 60 req/min
- ‚ùå Empathy/medical tone ch∆∞a b·∫±ng GPT-4

**Use case:** MVP, testing, d·ª± √°n nh·ªè

---

### Option 3: Local LLM (Llama 3, Mistral)

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Ho√†n to√†n free
- ‚úÖ Privacy t·ªët (kh√¥ng g·ª≠i data ra ngo√†i)
- ‚úÖ Kh√¥ng gi·ªõi h·∫°n requests

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå C·∫ßn GPU m·∫°nh (8GB+ VRAM)
- ‚ùå Ch·∫•t l∆∞·ª£ng ti·∫øng Vi·ªát k√©m h∆°n
- ‚ùå Setup ph·ª©c t·∫°p h∆°n

**Use case:** Privacy-focused, budget th·∫•p, c√≥ infrastructure

---

## üöÄ Implementation Steps

### B∆∞·ªõc 1: T·∫°o Chatbot Service

```bash
# T·∫°o th∆∞ m·ª•c m·ªõi
mkdir chatbot-service
cd chatbot-service

# T·∫°o c·∫•u tr√∫c
mkdir -p chatbot_app/{prompts,utils}
touch chatbot_app/{__init__.py,main.py,schemas.py,conversation.py,llm_client.py}
touch requirements.txt Dockerfile .env.example
```

---

### B∆∞·ªõc 2: C√†i ƒë·∫∑t Dependencies

**requirements.txt:**
```txt
fastapi==0.115.0
uvicorn==0.32.0
pydantic==2.9.0
python-dotenv==1.0.0

# Option 1: OpenAI
openai==1.54.0

# Option 2: Google Gemini
google-generativeai==0.8.3

# Option 3: Local LLM
# langchain==0.3.7
# llama-cpp-python==0.3.2

# Utils
httpx==0.27.0
redis==5.2.0  # Optional: cache conversations
```

---

### B∆∞·ªõc 3: Code Implementation

#### **chatbot_app/schemas.py**
```python
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
from datetime import datetime

class AnalysisContext(BaseModel):
    """Context t·ª´ AI Service"""
    risk: str
    reason: str
    primary_disease: Optional[Dict] = None
    alternative_diseases: Optional[List[Dict]] = []
    recommendations: Optional[List[str]] = []
    clinical_concepts: Optional[List[str]] = []

class ChatMessage(BaseModel):
    role: str  # "user" | "assistant" | "system"
    content: str
    timestamp: datetime = Field(default_factory=datetime.now)

class ChatRequest(BaseModel):
    session_id: str
    message: str
    analysis_context: Optional[AnalysisContext] = None

class ChatResponse(BaseModel):
    session_id: str
    message: str
    suggestions: Optional[List[str]] = []  # G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo
```

---

#### **chatbot_app/prompts/system_prompt.py**
```python
SYSTEM_PROMPT = """
B·∫°n l√† tr·ª£ l√Ω AI y t·∫ø chuy√™n v·ªÅ da li·ªÖu c·ªßa DermaSafe-AI, m·ªôt h·ªá th·ªëng s√†ng l·ªçc r·ªßi ro da li·ªÖu.

NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
- Gi·∫£i th√≠ch k·∫øt qu·∫£ ph√¢n t√≠ch AI cho ng∆∞·ªùi d√πng m·ªôt c√°ch d·ªÖ hi·ªÉu
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ t√¨nh tr·∫°ng da c·ªßa h·ªç
- H·ªó tr·ª£, an ·ªßi v√† ƒë∆∞a ra l·ªùi khuy√™n h·ª£p l√Ω
- Lu√¥n nh·∫•n m·∫°nh: "ƒê√¢y KH√îNG ph·∫£i ch·∫©n ƒëo√°n y khoa ch√≠nh th·ª©c"

NGUY√äN T·∫ÆC QUAN TR·ªåNG:
1. ‚ö†Ô∏è KH√îNG BAO GI·ªú ch·∫©n ƒëo√°n ch√≠nh th·ª©c ho·∫∑c thay th·∫ø b√°c sƒ©
2. ‚úÖ Lu√¥n khuy√™n ng∆∞·ªùi d√πng ƒëi kh√°m b√°c sƒ© n·∫øu nghi ng·ªù
3. üáªüá≥ Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, gi·ªçng ƒëi·ªáu th√¢n thi·ªán, empathetic
4. üìä S·ª≠ d·ª•ng k·∫øt qu·∫£ AI analysis ƒë·ªÉ tr·∫£ l·ªùi (n·∫øu c√≥)
5. üéØ Ng·∫Øn g·ªçn, r√µ r√†ng, tr√°nh thu·∫≠t ng·ªØ ph·ª©c t·∫°p

TH√îNG TIN NGUY HI·ªÇM - PH·∫¢I C·∫¢NH B√ÅO NGAY:
- N·∫øu user m√¥ t·∫£: ch·∫£y m√°u kh√¥ng d·ª´ng, ƒëau d·ªØ d·ªôi, s·ªët cao, lan nhanh
- ‚Üí Tr·∫£ l·ªùi: "‚ö†Ô∏è Tri·ªáu ch·ª©ng n√†y c·∫ßn ƒëi b·ªánh vi·ªán NGAY L·∫¨P T·ª®C"

GI·ªåNG ƒêI·ªÜU:
- Th√¢n thi·ªán, ·∫•m √°p nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n quan t√¢m
- Kh√¥ng g√¢y ho·∫£ng lo·∫°n, nh∆∞ng c≈©ng kh√¥ng coi nh·∫π
- Khuy·∫øn kh√≠ch h√†nh ƒë·ªông t√≠ch c·ª±c (ƒëi kh√°m n·∫øu c·∫ßn)

CONTEXT S·∫º ƒê∆Ø·ª¢C CUNG C·∫§P:
- K·∫øt qu·∫£ ph√¢n t√≠ch AI (n·∫øu user ƒë√£ upload ·∫£nh)
- L·ªãch s·ª≠ tr√≤ chuy·ªán (conversation history)
"""

def build_context_prompt(analysis: dict) -> str:
    """T·∫°o prompt t·ª´ k·∫øt qu·∫£ AI analysis"""
    if not analysis:
        return ""
    
    prompt = f"""
K·∫æT QU·∫¢ PH√ÇN T√çCH AI (User v·ª´a nh·∫≠n ƒë∆∞·ª£c):
- M·ª©c ƒë·ªô r·ªßi ro: {analysis.get('risk', 'N/A').upper()}
- L√Ω do: {analysis.get('reason', 'N/A')}
"""
    
    if analysis.get('primary_disease'):
        disease = analysis['primary_disease']
        prompt += f"""
- Ch·∫©n ƒëo√°n ch√≠nh: {disease.get('vietnamese_name', 'N/A')} ({disease.get('confidence', 0)*100:.1f}%)
- M·ª©c ƒë·ªô nghi√™m tr·ªçng: {disease.get('severity', 'N/A')}
"""
    
    if analysis.get('recommendations'):
        prompt += "\nKHUY·∫æN NGH·ªä:\n"
        for rec in analysis['recommendations'][:3]:
            prompt += f"  ‚Ä¢ {rec}\n"
    
    return prompt
```

---

#### **chatbot_app/llm_client.py**
```python
import os
from typing import List, Dict
from openai import OpenAI
# ho·∫∑c: import google.generativeai as genai

class LLMClient:
    def __init__(self, provider: str = "openai"):
        """
        provider: "openai" | "gemini" | "local"
        """
        self.provider = provider
        
        if provider == "openai":
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.model = "gpt-4o-mini"  # R·∫ª h∆°n, v·∫´n t·ªët
        
        elif provider == "gemini":
            import google.generativeai as genai
            genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
            self.model = genai.GenerativeModel('gemini-pro')
        
        # elif provider == "local":
        #     from llama_cpp import Llama
        #     self.client = Llama(model_path="models/llama3-8b.gguf")
    
    async def chat(
        self, 
        messages: List[Dict[str, str]], 
        temperature: float = 0.7
    ) -> str:
        """
        G·ª≠i messages v√† nh·∫≠n response t·ª´ LLM
        
        messages format:
        [
            {"role": "system", "content": "You are..."},
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there!"},
            {"role": "user", "content": "How are you?"}
        ]
        """
        
        if self.provider == "openai":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=500  # Gi·ªõi h·∫°n ƒë·ªô d√†i response
            )
            return response.choices[0].message.content
        
        elif self.provider == "gemini":
            # Gemini format kh√°c m·ªôt ch√∫t
            chat = self.model.start_chat(history=[])
            for msg in messages:
                if msg['role'] == 'user':
                    response = chat.send_message(msg['content'])
            return response.text
        
        else:
            raise ValueError(f"Unknown provider: {self.provider}")
```

---

#### **chatbot_app/conversation.py**
```python
from typing import List, Dict, Optional
from .schemas import ChatMessage, AnalysisContext
from .prompts.system_prompt import SYSTEM_PROMPT, build_context_prompt

class ConversationManager:
    """Qu·∫£n l√Ω conversation history cho m·ªói session"""
    
    def __init__(self):
        # ƒê∆°n gi·∫£n: l∆∞u trong memory
        # Production: d√πng Redis ho·∫∑c database
        self.sessions: Dict[str, List[ChatMessage]] = {}
    
    def get_messages(self, session_id: str) -> List[ChatMessage]:
        """L·∫•y l·ªãch s·ª≠ chat"""
        return self.sessions.get(session_id, [])
    
    def add_message(self, session_id: str, message: ChatMessage):
        """Th√™m message v√†o l·ªãch s·ª≠"""
        if session_id not in self.sessions:
            self.sessions[session_id] = []
        self.sessions[session_id].append(message)
    
    def build_llm_messages(
        self, 
        session_id: str,
        analysis_context: Optional[AnalysisContext] = None
    ) -> List[Dict[str, str]]:
        """
        Chuy·ªÉn conversation history th√†nh format cho LLM
        """
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        
        # Th√™m context t·ª´ AI analysis (n·∫øu c√≥)
        if analysis_context:
            context_prompt = build_context_prompt(analysis_context.model_dump())
            messages.append({
                "role": "system", 
                "content": context_prompt
            })
        
        # Th√™m l·ªãch s·ª≠ chat (gi·ªõi h·∫°n 10 messages g·∫ßn nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám tokens)
        history = self.get_messages(session_id)[-10:]
        for msg in history:
            if msg.role in ["user", "assistant"]:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        return messages
    
    def clear_session(self, session_id: str):
        """X√≥a l·ªãch s·ª≠ chat"""
        if session_id in self.sessions:
            del self.sessions[session_id]
```

---

#### **chatbot_app/main.py**
```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from typing import List
import uuid
from datetime import datetime

from .schemas import (
    ChatRequest, 
    ChatResponse, 
    ChatMessage,
    AnalysisContext
)
from .conversation import ConversationManager
from .llm_client import LLMClient
import os

app = FastAPI(title="DermaSafe Chatbot Service", version="1.0.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize
conversation_manager = ConversationManager()
llm_provider = os.getenv("LLM_PROVIDER", "openai")  # "openai" | "gemini"
llm_client = LLMClient(provider=llm_provider)

@app.get("/health")
async def health():
    return {
        "status": "ok",
        "llm_provider": llm_provider
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Main chatbot endpoint
    """
    # L∆∞u message c·ªßa user
    user_message = ChatMessage(
        role="user",
        content=request.message
    )
    conversation_manager.add_message(request.session_id, user_message)
    
    # Build messages cho LLM
    llm_messages = conversation_manager.build_llm_messages(
        session_id=request.session_id,
        analysis_context=request.analysis_context
    )
    
    # G·ªçi LLM
    try:
        assistant_reply = await llm_client.chat(
            messages=llm_messages,
            temperature=0.7
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"LLM Error: {str(e)}"
        )
    
    # L∆∞u response c·ªßa assistant
    assistant_message = ChatMessage(
        role="assistant",
        content=assistant_reply
    )
    conversation_manager.add_message(request.session_id, assistant_message)
    
    # T·∫°o suggestions (optional)
    suggestions = generate_suggestions(request.analysis_context)
    
    return ChatResponse(
        session_id=request.session_id,
        message=assistant_reply,
        suggestions=suggestions
    )

@app.get("/chat/history/{session_id}", response_model=List[ChatMessage])
async def get_history(session_id: str):
    """L·∫•y l·ªãch s·ª≠ chat"""
    return conversation_manager.get_messages(session_id)

@app.delete("/chat/history/{session_id}")
async def clear_history(session_id: str):
    """X√≥a l·ªãch s·ª≠ chat"""
    conversation_manager.clear_session(session_id)
    return {"message": "History cleared"}

def generate_suggestions(context: AnalysisContext | None) -> List[str]:
    """T·∫°o g·ª£i √Ω c√¢u h·ªèi d·ª±a tr√™n context"""
    if not context:
        return [
            "T√¥i n√™n l√†m g√¨ ti·∫øp theo?",
            "T√¨nh tr·∫°ng n√†y c√≥ nguy hi·ªÉm kh√¥ng?",
            "T√¥i c·∫ßn ƒëi kh√°m b√°c sƒ© kh√¥ng?"
        ]
    
    suggestions = []
    
    if context.risk == "cao":
        suggestions = [
            "T√¥i n√™n ƒëi kh√°m ngay hay ch·ªù v√†i ng√†y?",
            "C√≥ c√°ch n√†o gi·∫£m tri·ªáu ch·ª©ng kh√¥ng?",
            "B·ªánh n√†y c√≥ ch·ªØa ƒë∆∞·ª£c kh√¥ng?"
        ]
    elif context.risk == "trung b√¨nh":
        suggestions = [
            "T√¥i n√™n theo d√µi nh·ªØng tri·ªáu ch·ª©ng n√†o?",
            "Khi n√†o t√¥i c·∫ßn ƒëi kh√°m?",
            "C√≥ c√°ch ph√≤ng ng·ª´a kh√¥ng?"
        ]
    else:
        suggestions = [
            "L√†m sao ƒë·ªÉ chƒÉm s√≥c da t·ªët h∆°n?",
            "T√¥i c√≥ n√™n theo d√µi th√™m kh√¥ng?",
            "Khi n√†o c·∫ßn ƒëi kh√°m b√°c sƒ©?"
        ]
    
    return suggestions
```

---

### B∆∞·ªõc 4: Frontend Integration

#### **frontend/src/components/ChatInterface.tsx** (NEW)
```tsx
import { useState, useEffect, useRef } from 'react';
import { v4 as uuidv4 } from 'uuid';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

interface ChatInterfaceProps {
  analysisContext?: any;  // T·ª´ AI analysis result
}

export default function ChatInterface({ analysisContext }: ChatInterfaceProps) {
  const [sessionId] = useState(() => uuidv4());
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const [suggestions, setSuggestions] = useState<string[]>([]);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const sendMessage = async (message: string) => {
    if (!message.trim()) return;

    // Add user message
    const userMessage: Message = {
      role: 'user',
      content: message,
      timestamp: new Date()
    };
    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setLoading(true);

    try {
      const response = await fetch('/api/v1/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          session_id: sessionId,
          message: message,
          analysis_context: analysisContext
        })
      });

      const data = await response.json();

      // Add assistant message
      const assistantMessage: Message = {
        role: 'assistant',
        content: data.message,
        timestamp: new Date()
      };
      setMessages(prev => [...prev, assistantMessage]);
      
      if (data.suggestions) {
        setSuggestions(data.suggestions);
      }
    } catch (error) {
      console.error('Chat error:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="w-full max-w-2xl bg-white rounded-xl shadow-lg flex flex-col h-[600px]">
      {/* Header */}
      <div className="bg-gradient-to-r from-indigo-500 to-purple-500 text-white p-4 rounded-t-xl">
        <h2 className="text-xl font-bold">üí¨ Chat v·ªõi AI H·ªó tr·ª£</h2>
        <p className="text-sm opacity-90">H·ªèi ƒë√°p v·ªÅ k·∫øt qu·∫£ ph√¢n t√≠ch c·ªßa b·∫°n</p>
      </div>

      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.length === 0 && (
          <div className="text-center text-gray-500 py-8">
            <p className="mb-2">üëã Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?</p>
            <p className="text-sm">H√£y h·ªèi t√¥i v·ªÅ k·∫øt qu·∫£ ph√¢n t√≠ch ho·∫∑c b·∫•t k·ª≥ th·∫Øc m·∫Øc n√†o v·ªÅ da li·ªÖu.</p>
          </div>
        )}

        {messages.map((msg, idx) => (
          <div
            key={idx}
            className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}
          >
            <div
              className={`max-w-[80%] rounded-lg p-3 ${
                msg.role === 'user'
                  ? 'bg-indigo-500 text-white'
                  : 'bg-gray-100 text-gray-800'
              }`}
            >
              <p className="whitespace-pre-wrap">{msg.content}</p>
              <span className="text-xs opacity-70 mt-1 block">
                {msg.timestamp.toLocaleTimeString('vi-VN')}
              </span>
            </div>
          </div>
        ))}

        {loading && (
          <div className="flex justify-start">
            <div className="bg-gray-100 rounded-lg p-3">
              <div className="flex space-x-2">
                <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce"></div>
                <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce delay-100"></div>
                <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce delay-200"></div>
              </div>
            </div>
          </div>
        )}

        <div ref={messagesEndRef} />
      </div>

      {/* Suggestions */}
      {suggestions.length > 0 && (
        <div className="px-4 pb-2 flex flex-wrap gap-2">
          {suggestions.map((suggestion, idx) => (
            <button
              key={idx}
              onClick={() => sendMessage(suggestion)}
              className="text-sm bg-gray-100 hover:bg-gray-200 text-gray-700 px-3 py-1 rounded-full transition"
            >
              üí° {suggestion}
            </button>
          ))}
        </div>
      )}

      {/* Input */}
      <div className="p-4 border-t">
        <div className="flex gap-2">
          <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            onKeyPress={(e) => e.key === 'Enter' && sendMessage(input)}
            placeholder="Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."
            className="flex-1 px-4 py-2 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-indigo-500"
            disabled={loading}
          />
          <button
            onClick={() => sendMessage(input)}
            disabled={loading || !input.trim()}
            className="px-6 py-2 bg-indigo-500 text-white rounded-lg hover:bg-indigo-600 disabled:opacity-50 transition"
          >
            G·ª≠i
          </button>
        </div>
      </div>
    </div>
  );
}
```

#### **Update App.tsx:**
```tsx
import ChatInterface from './components/ChatInterface';

// ... existing code ...

{result && (
  <>
    <ResultCard risk={result.risk} reason={result.reason} />
    
    {/* NEW: Chat interface */}
    <div className="mt-6">
      <ChatInterface analysisContext={result} />
    </div>
  </>
)}
```

---

### B∆∞·ªõc 5: Backend API Proxy (Optional)

**backend-api/backend_app/main.py** - Th√™m proxy cho chatbot:
```python
@app.post("/api/v1/chat")
async def chat(request: dict):
    """Proxy chatbot requests"""
    async with httpx.AsyncClient(
        base_url="http://localhost:8002",
        timeout=30.0
    ) as client:
        r = await client.post("/chat", json=request)
        r.raise_for_status()
        return r.json()
```

---

### B∆∞·ªõc 6: Docker Setup

**chatbot-service/Dockerfile:**
```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code
COPY chatbot_app/ ./chatbot_app/

# Run
CMD ["uvicorn", "chatbot_app.main:app", "--host", "0.0.0.0", "--port", "8002"]
```

**docker-compose.yml** - Th√™m service:
```yaml
services:
  # ... existing services ...
  
  chatbot:
    build: ./chatbot-service
    ports:
      - "8002:8002"
    environment:
      - LLM_PROVIDER=openai  # or gemini
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    depends_on:
      - ai-service
```

---

## üí∞ Chi ph√≠ ∆∞·ªõc t√≠nh

### OpenAI GPT-4o-mini
- Input: $0.15 / 1M tokens
- Output: $0.60 / 1M tokens
- Trung b√¨nh 1 conversation (10 messages): ~2000 tokens
- **Chi ph√≠**: ~$0.002/conversation ‚âà 50ƒë/conversation
- **1000 users/ng√†y**: ~50k VNƒê/ng√†y ‚âà 1.5M VNƒê/th√°ng

### Google Gemini Pro
- **Free tier**: 60 requests/minute = 86,400 requests/ng√†y
- **Paid**: $0.00025/1K characters (~$0.001/conversation)
- **Chi ph√≠**: G·∫ßn nh∆∞ mi·ªÖn ph√≠ cho d·ª± √°n nh·ªè

### Local LLM (Llama 3)
- **Chi ph√≠**: $0 (ch·ªâ t·ªën ƒëi·ªán + infrastructure)
- **Infrastructure**: VPS GPU (~$50-100/th√°ng)

---

## üìä So s√°nh c√°c ph∆∞∆°ng √°n

| Ti√™u ch√≠ | OpenAI GPT-4o-mini | Google Gemini | Local LLM |
|----------|-------------------|---------------|-----------|
| Ch·∫•t l∆∞·ª£ng ti·∫øng Vi·ªát | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Chi ph√≠ (1000 users/ng√†y) | ~1.5M/th√°ng | Free | ~2M/th√°ng (GPU server) |
| T·ªëc ƒë·ªô ph·∫£n h·ªìi | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Privacy | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| ƒê·ªô kh√≥ setup | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| Empathy/Medical tone | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

**Khuy·∫øn ngh·ªã:**
- **MVP/Testing**: Gemini (free tier)
- **Production nh·ªè**: GPT-4o-mini (t·ªët + r·∫ª)
- **Scale l·ªõn**: Local LLM (n·∫øu c√≥ infrastructure)

---

## üß™ Testing

```bash
# Test chatbot service
curl -X POST http://localhost:8002/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "test-123",
    "message": "K·∫øt qu·∫£ n√†y c√≥ nghƒ©a l√† g√¨?",
    "analysis_context": {
      "risk": "cao",
      "reason": "Ph√°t hi·ªán t·ªïn th∆∞∆°ng nguy hi·ªÉm",
      "primary_disease": {
        "vietnamese_name": "Ung th∆∞ h·∫Øc t·ªë",
        "confidence": 0.72
      }
    }
  }'
```

---

## üöÄ Deployment Checklist

- [ ] T·∫°o OpenAI/Gemini API key
- [ ] Setup environment variables
- [ ] Test locally v·ªõi `uvicorn`
- [ ] Test v·ªõi Docker Compose
- [ ] Implement rate limiting (tr√°nh spam)
- [ ] Setup monitoring (Sentry, Datadog)
- [ ] Th√™m logging conversation (GDPR compliant)
- [ ] Test empathy v√† medical accuracy
- [ ] A/B test different LLMs
- [ ] Setup cache (Redis) cho sessions

---

## üìö Resources

- [OpenAI API Docs](https://platform.openai.com/docs)
- [Google Gemini Docs](https://ai.google.dev/docs)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/tutorial/)
- [LangChain](https://python.langchain.com/) - Framework cho complex chatbots

---

## üéØ Roadmap

**Phase 1: MVP (1-2 tu·∫ßn)**
- ‚úÖ Basic chatbot v·ªõi Gemini free tier
- ‚úÖ Simple conversation memory
- ‚úÖ Integration v·ªõi AI analysis results

**Phase 2: Enhanced (2-3 tu·∫ßn)**
- ‚¨ú Switch to GPT-4o-mini
- ‚¨ú Redis cache cho sessions
- ‚¨ú Streaming responses (real-time typing)
- ‚¨ú Sentiment analysis (ph√°t hi·ªán user anxiety)

**Phase 3: Advanced (1-2 th√°ng)**
- ‚¨ú Function calling (book doctor appointments)
- ‚¨ú Multimodal (send images in chat)
- ‚¨ú Voice input/output
- ‚¨ú Personalized recommendations based on history

---

**üéâ Good luck v·ªõi chatbot! N·∫øu c·∫ßn support, ping t√¥i nh√©!**
